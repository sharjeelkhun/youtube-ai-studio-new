import { GoogleGenerativeAI } from '@google/generative-ai'
import OpenAI from 'openai'
import Anthropic from '@anthropic-ai/sdk'
import Mistral from '@mistralai/mistralai'
import { formatAndCleanDescription } from './prompts'
import { formatAndCleanDescription } from './prompts'
import { formatAndCleanDescription } from './prompts'

interface AIResponse {
  title?: string
  description?: string
}

// Shared response processing function
function processResponse(input: any): AIResponse {
  if (!input) return {}

  try {
    if (typeof input === 'string') {
      // Try to parse as JSON if it's a string
      try {
        const parsed = JSON.parse(input)
        return {
          title: parsed.title,
          description: parsed.description
        }
      } catch {
        // If not JSON, assume it's just the description
        return { description: input }
      }
    } else if (typeof input === 'object') {
      return {
        title: input.title,
        description: input.description
      }
    }
  } catch (error) {
    console.error('Error processing AI response:', error)
    throw new Error('Failed to process AI response')
  }

  return {}
}

interface AiSettings {
  defaultModel: string
  temperature: 'precise' | 'balanced' | 'creative'
}

const temperatureMap = {
  precise: 0.2,
  balanced: 0.7,
  creative: 1.0,
}

export async function handleGemini(apiKey: string, title: string, description: string, settings: AiSettings) {
  const genAI = new GoogleGenerativeAI(apiKey)
  const model = genAI.getGenerativeModel({ model: settings.defaultModel || 'gemini-pro' })

  const prompt = formatAndCleanDescription(title, description)

  const result = await model.generateContent(prompt)
  const response = result.response
  const text = response.text()

  return processResponse(text)
}

export async function handleOpenAI(apiKey: string, title: string, description: string, settings: AiSettings) {
  const openai = new OpenAI({ apiKey })

  const prompt = formatAndCleanDescription(title, description)

  const completion = await openai.chat.completions.create({
    model: settings.defaultModel || 'gpt-3.5-turbo',
    temperature: temperatureMap[settings.temperature || 'balanced'],
    messages: [
      {
        role: 'system',
        content: 'You are a YouTube content optimization specialist.'
      },
      {
        role: 'user',
        content: prompt
      }
    ]
  })

  return processResponse(completion.choices[0]?.message?.content)
}

export async function handleAnthropic(apiKey: string, title: string, description: string, settings: AiSettings) {
  const anthropic = new Anthropic({ apiKey })

  const prompt = formatAndCleanDescription(title, description)

  const message = await anthropic.messages.create({
    model: settings.defaultModel || 'claude-3-opus-20240229',
    max_tokens: 4096,
    temperature: temperatureMap[settings.temperature || 'balanced'],
    messages: [
      {
        role: 'user',
        content: prompt
      }
    ]
  })

  return processResponse(message.content[0].text)
}

export async function handleMistral(apiKey: string, title: string, description: string, settings: AiSettings) {
  const client = new MistralClient(apiKey)

  const prompt = formatAndCleanDescription(title, description)

  const response = await client.chat({
    model: settings.defaultModel || 'mistral-large-latest',
    temperature: temperatureMap[settings.temperature || 'balanced'],
    messages: [
      {
        role: 'system',
        content: 'You are a YouTube content optimization specialist.'
      },
      {
        role: 'user',
        content: prompt
      }
    ]
  })

  return processResponse(response.choices[0]?.message?.content)
}
